{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class AttentionModule(layers.Layer):\n",
    "    \"\"\"\n",
    "    Self-attention mechanism for feature refinement\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionModule, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.channels = input_shape[-1]\n",
    "        self.query_conv = layers.Conv2D(self.channels // 8, kernel_size=1)\n",
    "        self.key_conv = layers.Conv2D(self.channels // 8, kernel_size=1)\n",
    "        self.value_conv = layers.Conv2D(self.channels, kernel_size=1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        height, width = inputs.shape[1], inputs.shape[2]\n",
    "\n",
    "        # Query, Key, Value projections\n",
    "        query = self.query_conv(inputs)\n",
    "        key = self.key_conv(inputs)\n",
    "        value = self.value_conv(inputs)\n",
    "\n",
    "        # Reshape for attention computation\n",
    "        query = tf.reshape(query, [batch_size, -1, self.channels // 8])\n",
    "        key = tf.reshape(key, [batch_size, -1, self.channels // 8])\n",
    "        value = tf.reshape(value, [batch_size, -1, self.channels])\n",
    "\n",
    "        # Compute attention scores\n",
    "        attention_weights = tf.matmul(query, key, transpose_b=True)\n",
    "        scale = tf.sqrt(tf.cast(self.channels // 8, tf.float32))\n",
    "        attention_weights = tf.nn.softmax(attention_weights / scale)\n",
    "\n",
    "        # Apply attention to values\n",
    "        attention_output = tf.matmul(attention_weights, value)\n",
    "        return tf.reshape(attention_output, [batch_size, height, width, self.channels])\n"
   ],
   "id": "79893fc3899cf2ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class MultiscaleModule(layers.Layer):\n",
    "    \"\"\"\n",
    "    Multiscale feature learning module with multiple receptive fields\n",
    "    \"\"\"\n",
    "    def __init__(self, filters, **kwargs):\n",
    "        super(MultiscaleModule, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Different scale convolutions\n",
    "        self.conv1 = layers.Conv2D(self.filters // 4, kernel_size=1, padding='same')\n",
    "        self.conv3 = layers.Conv2D(self.filters // 4, kernel_size=3, padding='same')\n",
    "        self.conv5 = layers.Conv2D(self.filters // 4, kernel_size=5, padding='same')\n",
    "        self.conv7 = layers.Conv2D(self.filters // 4, kernel_size=7, padding='same')\n",
    "        \n",
    "        # Batch normalization layers\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.bn5 = layers.BatchNormalization()\n",
    "        self.bn7 = layers.BatchNormalization()\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Process at different scales\n",
    "        scale1 = self.bn1(self.conv1(inputs), training=training)\n",
    "        scale3 = self.bn3(self.conv3(inputs), training=training)\n",
    "        scale5 = self.bn5(self.conv5(inputs), training=training)\n",
    "        scale7 = self.bn7(self.conv7(inputs), training=training)\n",
    "\n",
    "        # Activate all scales\n",
    "        scale1 = tf.nn.relu(scale1)\n",
    "        scale3 = tf.nn.relu(scale3)\n",
    "        scale5 = tf.nn.relu(scale5)\n",
    "        scale7 = tf.nn.relu(scale7)\n",
    "\n",
    "        # Concatenate all scales\n",
    "        return tf.concat([scale1, scale3, scale5, scale7], axis=-1)\n"
   ],
   "id": "6225f244408e342",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def build_model(input_shape=(28, 28, 1), num_classes=10):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # 1. Initial Convolution\n",
    "    x = layers.Conv2D(32, kernel_size=3, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    # 2. Attention Mechanism\n",
    "    attention_output = AttentionModule()(x)\n",
    "    \n",
    "    # 3. Multiscale Feature Learning\n",
    "    multiscale_features = MultiscaleModule(64)(attention_output)\n",
    "    \n",
    "    # Save for residual connection\n",
    "    residual = multiscale_features\n",
    "    \n",
    "    # 4. Residual Convolution Block\n",
    "    x = layers.Conv2D(64, kernel_size=3, padding='same')(multiscale_features)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(64, kernel_size=3, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Add residual connection\n",
    "    x = layers.Add()([x, residual])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    # 5. Global Average Pooling and Classification\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return models.Model(inputs, outputs)"
   ],
   "id": "f66e93ec02cbba24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ],
   "id": "97212152b4dc9e16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = build_model()\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ],
   "id": "674c9238438409ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.summary()",
   "id": "634b23f233377776",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=64,\n",
    "    epochs=2,\n",
    "    validation_split=0.2\n",
    ")"
   ],
   "id": "edbb07caf9c9fe53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m750/750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m219s\u001B[0m 288ms/step - accuracy: 0.5548 - loss: 1.2393 - val_accuracy: 0.2738 - val_loss: 5.4314\n",
      "Epoch 2/2\n",
      "\u001B[1m750/750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m216s\u001B[0m 288ms/step - accuracy: 0.8721 - loss: 0.4021 - val_accuracy: 0.1722 - val_loss: 9.4540\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T10:40:08.165560Z",
     "start_time": "2025-01-22T10:39:55.564861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"\\nTest accuracy: {test_accuracy:.4f}\")"
   ],
   "id": "3485d106735ff32f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 40ms/step - accuracy: 0.1737 - loss: 9.4431\n",
      "\n",
      "Test accuracy: 0.1701\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c1f2535cc8147bb0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
